{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c916c4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-12-21T15:58:01.516415Z",
     "iopub.status.busy": "2022-12-21T15:58:01.515208Z",
     "iopub.status.idle": "2022-12-21T15:58:01.552870Z",
     "shell.execute_reply": "2022-12-21T15:58:01.551812Z"
    },
    "papermill": {
     "duration": 0.050827,
     "end_time": "2022-12-21T15:58:01.557011",
     "exception": false,
     "start_time": "2022-12-21T15:58:01.506184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\n",
      "/kaggle/input/sample/scraped_tweets.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8744164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:58:01.572186Z",
     "iopub.status.busy": "2022-12-21T15:58:01.571337Z",
     "iopub.status.idle": "2022-12-21T15:58:04.188657Z",
     "shell.execute_reply": "2022-12-21T15:58:04.187437Z"
    },
    "papermill": {
     "duration": 2.627861,
     "end_time": "2022-12-21T15:58:04.191694",
     "exception": false,
     "start_time": "2022-12-21T15:58:01.563833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download(\"movie_reviews\")\n",
    "import numpy as np\n",
    "from nltk.corpus import movie_reviews \n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples \n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop = stopwords.words('english')\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383b15b",
   "metadata": {
    "papermill": {
     "duration": 0.006445,
     "end_time": "2022-12-21T15:58:04.204771",
     "exception": false,
     "start_time": "2022-12-21T15:58:04.198326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper Methods for Cleaning Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf851dfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:58:04.220266Z",
     "iopub.status.busy": "2022-12-21T15:58:04.219765Z",
     "iopub.status.idle": "2022-12-21T15:58:04.237429Z",
     "shell.execute_reply": "2022-12-21T15:58:04.235759Z"
    },
    "papermill": {
     "duration": 0.029031,
     "end_time": "2022-12-21T15:58:04.240228",
     "exception": false,
     "start_time": "2022-12-21T15:58:04.211197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# code here\n",
    "def remove_all_punctuations(data):\n",
    "    all_punctuations =  list(string.punctuation)\n",
    "    all_punctuations.append(\"``\")\n",
    "    lst = []\n",
    "    for review in data:\n",
    "        tokenized = word_tokenize(review)\n",
    "        considered_word = [word for word in tokenized if not word in all_punctuations]\n",
    "        considered_word = \" \".join(considered_word)\n",
    "        lst.append(considered_word)\n",
    "    return lst\n",
    "\n",
    "def remove_hashtags(data):\n",
    "        data= [review.replace(\"#\", '\"') for review in data]\n",
    "        return data\n",
    "\n",
    "# code here\n",
    "def remove_stop_words(data):\n",
    "    stop_words = set(stop)\n",
    "    lst = []\n",
    "    for review in data:\n",
    "        tokenized = word_tokenize(review)\n",
    "        considered_word = [word for word in tokenized if not word in stop_words]\n",
    "        considered_word = \" \".join(considered_word)\n",
    "        lst.append(considered_word)\n",
    "    return lst\n",
    "\n",
    "def remove_links(data):\n",
    "    ans = []\n",
    "    for d in data:\n",
    "         ans.append(re.sub(r\"http\\S+\", \"\", d))\n",
    "    return ans\n",
    "\n",
    "def clean_html(data):\n",
    "    ans = []\n",
    "    html = re.compile('<.*?>')\n",
    "    for text in data:\n",
    "        \n",
    "        ans.append( html.sub(r'', text) )\n",
    "    return ans\n",
    "\n",
    "def remove_non_ascii(data):\n",
    "    ans =[]\n",
    "    for s in data:\n",
    "        ans.append(\"\".join(i for i in s if ord(i)<128))\n",
    "    return ans\n",
    "\n",
    "def clean_links(data):\n",
    "    ans = []\n",
    "    txt = re.compile('http(s)?://\\w+(\\.\\w+){1,}(/\\w+)*')\n",
    "    for text in data:\n",
    "        ans.append(txt.sub(r'', text))\n",
    "    return ans\n",
    "\n",
    "def clean_links_a1(data):\n",
    "    ans = []\n",
    "    txt = re.compile(r'//t.co/[a-zA-Z0-9_]*')\n",
    "    for text in data:\n",
    "        ans.append(txt.sub(r'',text))\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a063a",
   "metadata": {
    "papermill": {
     "duration": 0.006753,
     "end_time": "2022-12-21T15:58:04.253868",
     "exception": false,
     "start_time": "2022-12-21T15:58:04.247115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reading Data & Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b212ed5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:58:04.270031Z",
     "iopub.status.busy": "2022-12-21T15:58:04.269447Z",
     "iopub.status.idle": "2022-12-21T15:58:13.465939Z",
     "shell.execute_reply": "2022-12-21T15:58:13.464526Z"
    },
    "papermill": {
     "duration": 9.209279,
     "end_time": "2022-12-21T15:58:13.469480",
     "exception": false,
     "start_time": "2022-12-21T15:58:04.260201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment         ids                          date      flag  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "dataset = pd.read_csv('/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv', encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n",
    "df = dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "772c6943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:58:13.484716Z",
     "iopub.status.busy": "2022-12-21T15:58:13.483719Z",
     "iopub.status.idle": "2022-12-21T15:58:14.104718Z",
     "shell.execute_reply": "2022-12-21T15:58:14.102944Z"
    },
    "papermill": {
     "duration": 0.632336,
     "end_time": "2022-12-21T15:58:14.108270",
     "exception": false,
     "start_time": "2022-12-21T15:58:13.475934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280000,)\n",
      "(320000,)\n",
      "(1280000,)\n",
      "(320000,)\n"
     ]
    }
   ],
   "source": [
    "X = df['text']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "print(X_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(y_dev.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a203d76e",
   "metadata": {
    "papermill": {
     "duration": 0.007692,
     "end_time": "2022-12-21T15:58:14.122577",
     "exception": false,
     "start_time": "2022-12-21T15:58:14.114885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cleaning TEXT for NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e71f10c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:58:14.137178Z",
     "iopub.status.busy": "2022-12-21T15:58:14.136654Z",
     "iopub.status.idle": "2022-12-21T16:09:46.166924Z",
     "shell.execute_reply": "2022-12-21T16:09:46.165295Z"
    },
    "papermill": {
     "duration": 692.044319,
     "end_time": "2022-12-21T16:09:46.173130",
     "exception": false,
     "start_time": "2022-12-21T15:58:14.128811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed HashTags\n",
      "Removed Links\n",
      "Removed Links Part 2\n",
      "Removed Stopwords\n",
      "Removed Punctuations\n",
      "Removed HashTags\n",
      "Removed Links\n",
      "Removed Links Part 2\n",
      "Removed Stopwords\n",
      "Removed Punctuations\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test = remove_hashtags(X_test)\n",
    "print(\"Removed HashTags\")\n",
    "X_test = clean_links(X_test)\n",
    "print(\"Removed Links\")\n",
    "#//t.co/DIID0JGmaX\n",
    "X_test = clean_links_a1(X_test)\n",
    "print(\"Removed Links Part 2\")\n",
    "X_test = remove_stop_words(X_test)\n",
    "print(\"Removed Stopwords\")\n",
    "X_test = remove_all_punctuations(X_test)\n",
    "print(\"Removed Punctuations\")\n",
    "\n",
    "X_dev = remove_hashtags(X_dev)\n",
    "print(\"Removed HashTags\")\n",
    "X_dev = clean_links(X_dev)\n",
    "print(\"Removed Links\")\n",
    "#//t.co/DIID0JGmaX\n",
    "X_dev = clean_links_a1(X_dev)\n",
    "print(\"Removed Links Part 2\")\n",
    "X_dev = remove_stop_words(X_dev)\n",
    "print(\"Removed Stopwords\")\n",
    "X_dev = remove_all_punctuations(X_dev)\n",
    "print(\"Removed Punctuations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb08f28",
   "metadata": {
    "papermill": {
     "duration": 0.006679,
     "end_time": "2022-12-21T16:09:46.186842",
     "exception": false,
     "start_time": "2022-12-21T16:09:46.180163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lemmatizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7fa6499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T16:09:46.202859Z",
     "iopub.status.busy": "2022-12-21T16:09:46.202349Z",
     "iopub.status.idle": "2022-12-21T16:19:50.424458Z",
     "shell.execute_reply": "2022-12-21T16:19:50.422567Z"
    },
    "papermill": {
     "duration": 604.238635,
     "end_time": "2022-12-21T16:19:50.432432",
     "exception": false,
     "start_time": "2022-12-21T16:09:46.193797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting lemmatization \n",
      "Xdev lemmatization done\n",
      "Xtestlemmatization done\n",
      "Lemmatization Completed\n"
     ]
    }
   ],
   "source": [
    "#code here\n",
    "#Ref : From lecture notest\n",
    "def stemSentence(sentence):\n",
    "         porter = PorterStemmer()\n",
    "         token_words = word_tokenize(sentence)\n",
    "         stem_sentence = [porter.stem(word) for word in token_words]\n",
    "         return \" \".join(stem_sentence.copy())\n",
    "print(\"Starting lemmatization \")\n",
    "\n",
    "X_dev = [stemSentence(review) for review in X_dev ]\n",
    "print(\"Xdev lemmatization done\")\n",
    "X_test = [stemSentence(review) for review in X_test ]\n",
    "print(\"Xtestlemmatization done\")\n",
    "\n",
    "print(\"Lemmatization Completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264735e6",
   "metadata": {
    "papermill": {
     "duration": 0.007066,
     "end_time": "2022-12-21T16:19:50.446554",
     "exception": false,
     "start_time": "2022-12-21T16:19:50.439488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8eba650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T16:19:50.463418Z",
     "iopub.status.busy": "2022-12-21T16:19:50.462759Z",
     "iopub.status.idle": "2022-12-21T16:20:26.431197Z",
     "shell.execute_reply": "2022-12-21T16:20:26.429792Z"
    },
    "papermill": {
     "duration": 35.987381,
     "end_time": "2022-12-21T16:20:26.441173",
     "exception": false,
     "start_time": "2022-12-21T16:19:50.453792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done vectorization for X Dev\n",
      "Done vectorization for X test\n"
     ]
    }
   ],
   "source": [
    "#code here\n",
    "tfidf_vector = TfidfVectorizer()\n",
    "X_dev_tfidf = tfidf_vector.fit_transform(X_dev)\n",
    "print(\"Done vectorization for X Dev\")\n",
    "X_dev_tfidf = tfidf_vector.transform(X_dev)\n",
    "print(\"Done vectorization for X test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a48606",
   "metadata": {
    "papermill": {
     "duration": 0.006692,
     "end_time": "2022-12-21T16:20:26.455025",
     "exception": false,
     "start_time": "2022-12-21T16:20:26.448333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vanilla Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "759ee0fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T16:20:26.471553Z",
     "iopub.status.busy": "2022-12-21T16:20:26.471010Z",
     "iopub.status.idle": "2022-12-21T16:20:47.812944Z",
     "shell.execute_reply": "2022-12-21T16:20:47.811429Z"
    },
    "papermill": {
     "duration": 21.353789,
     "end_time": "2022-12-21T16:20:47.815962",
     "exception": false,
     "start_time": "2022-12-21T16:20:26.462173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done vectorization for X Dev\n",
      "Done vectorization for y dev\n"
     ]
    }
   ],
   "source": [
    "vector = CountVectorizer()\n",
    "X_dev_text = vector.fit_transform(X_dev)\n",
    "print(\"Done vectorization for X Dev\")\n",
    "X_test_text = vector.transform(X_test)\n",
    "print(\"Done vectorization for y dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911a75e",
   "metadata": {
    "papermill": {
     "duration": 0.007148,
     "end_time": "2022-12-21T16:20:47.830919",
     "exception": false,
     "start_time": "2022-12-21T16:20:47.823771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fit Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7c211b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T16:20:47.848908Z",
     "iopub.status.busy": "2022-12-21T16:20:47.848159Z",
     "iopub.status.idle": "2022-12-21T16:31:00.868541Z",
     "shell.execute_reply": "2022-12-21T16:31:00.866841Z"
    },
    "papermill": {
     "duration": 613.03288,
     "end_time": "2022-12-21T16:31:00.871371",
     "exception": false,
     "start_time": "2022-12-21T16:20:47.838491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "Objective value = -558627.693738\n",
      "nSV = 1003272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(random_state=0, tol=1e-05, verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  LinearSVC(random_state=0, tol=1e-5, verbose=1)\n",
    "model.fit(X_dev_text, y_dev)\n",
    "# save the model to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33c0a2",
   "metadata": {
    "papermill": {
     "duration": 0.014647,
     "end_time": "2022-12-21T16:31:00.900291",
     "exception": false,
     "start_time": "2022-12-21T16:31:00.885644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fit Tfidf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e12e28a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T16:31:00.931566Z",
     "iopub.status.busy": "2022-12-21T16:31:00.930958Z",
     "iopub.status.idle": "2022-12-21T16:31:45.363510Z",
     "shell.execute_reply": "2022-12-21T16:31:45.361718Z"
    },
    "papermill": {
     "duration": 44.453457,
     "end_time": "2022-12-21T16:31:45.368206",
     "exception": false,
     "start_time": "2022-12-21T16:31:00.914749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]......*\n",
      "optimization finished, #iter = 67\n",
      "Objective value = -612697.972553\n",
      "nSV = 1037116\n",
      "Logistic Regression Completed for TFIDF Phase\n"
     ]
    }
   ],
   "source": [
    "lr_tfid =  LinearSVC(random_state=0, tol=1e-5, verbose=1)\n",
    "lr_tfid.fit(X_dev_tfidf,y_dev)\n",
    "print(\"Logistic Regression Completed for TFIDF Phase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb486f75",
   "metadata": {
    "papermill": {
     "duration": 0.014651,
     "end_time": "2022-12-21T16:31:45.400300",
     "exception": false,
     "start_time": "2022-12-21T16:31:45.385649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Saving Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b78e7315",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T16:31:45.432598Z",
     "iopub.status.busy": "2022-12-21T16:31:45.432133Z",
     "iopub.status.idle": "2022-12-21T16:31:46.081801Z",
     "shell.execute_reply": "2022-12-21T16:31:46.080749Z"
    },
    "papermill": {
     "duration": 0.670145,
     "end_time": "2022-12-21T16:31:46.084593",
     "exception": false,
     "start_time": "2022-12-21T16:31:45.414448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = 'finalized_model_SVM.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "filename_tfidf = 'finalized_model_tfidf_SVM.sav'\n",
    "pickle.dump(model, open(filename_tfidf, 'wb'))\n",
    "\n",
    "pickle.dump(vector, open(\"vectorizer_SVM.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(tfidf_vector, open(\"vectorizer_tfidf_SVM.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38dcab64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T16:31:46.116327Z",
     "iopub.status.busy": "2022-12-21T16:31:46.115845Z",
     "iopub.status.idle": "2022-12-21T16:31:47.206861Z",
     "shell.execute_reply": "2022-12-21T16:31:47.205018Z"
    },
    "papermill": {
     "duration": 1.110994,
     "end_time": "2022-12-21T16:31:47.210183",
     "exception": false,
     "start_time": "2022-12-21T16:31:46.099189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tzip warning: name not matched: directory_path\r\n",
      "\r\n",
      "zip error: Nothing to do! (try: zip -r file.zip . -i directory_path)\r\n"
     ]
    }
   ],
   "source": [
    "## Using Pretrained Model\n",
    "directory_path = \"/kaggle/input/sample\"\n",
    "!zip -r file.zip directory_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da99dda",
   "metadata": {
    "papermill": {
     "duration": 0.014135,
     "end_time": "2022-12-21T16:31:47.238998",
     "exception": false,
     "start_time": "2022-12-21T16:31:47.224863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### End of file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2038.278561,
   "end_time": "2022-12-21T16:31:49.690109",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-21T15:57:51.411548",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
